
offlinetoonline

这个框架的目的是，在离线数据集上预训练，然后再与环境交互

在每篇论文，要搞清楚，离线数据集上梯度更新的次数，与环境交互的次数，以及每次交互梯度更新多少次

至于方法，要搞清楚，离线学习阶段的训练方式和在线学习的训练方式是相同还是不同。


但是这一系列的论文，都很少有讲清楚的，明明这些很重要，肯定要专门提呀，并且他们在在线学习阶段，没有一直训练下去，这很奇怪啊。

这些论文，都没有做出一个保证，我从哪个数据集出发，我都能达到最终参考的在线学习的算法(SAC)，达到最终的峰值，如果从随机数据集出发，
最终只能达到峰值的一半，这能说明这个方法的成功吗？又或者从专家数据集出发，能达到专家的水平，但是继续训练就不如从头训练的SAC后续的进步了，
这也是问题啊，离线的预训练不应该是为了更好的在线训练吗？


AWAC

论文里面说的，离线数据集梯度更新次数是25k，之后与环境交互0.5M次

里面最大的问题是，没有说清楚自己的损失函数，我需要在具体的源码里面找。

总之没说清楚，找不到具体怎么做的。

值得注意的是，模仿学习在mujoco中表现的比较好，应该是mujoco太简单了。

在后续的offlinetoonline的实验中，尽管离线学习没有达到论文里的水平，但是进入在线学习似乎好起来了，他的训练actor的方法是不区分正负的，大家都是利用，只是区分多少，这样子也能达到目的。

这里如果要转到在线，为什么还是仅仅用正的值去引导动作？为什么不直接用adv？


PEX

主要是将离线训练的actor新存一份不再训练，作为参考，然后继续训练之前离线训练的actor，用一个权重来综合他们之间的动作。

在这里做实验的时候，有个困惑，PEX在离线和在线中都跑了1e6，如果直接在线训练效果会好很多，但是这个框架切换到其他任务的效果又没有被检验，所以
很难受，感觉成了自定标准的游戏了。

好像有点问题，他在在线训练时，计算动作用的adv时，用的是新的动作，这里我没看到。


IQL

每天想到只用正的更新也荡上去了。



offline2online

有点问题,之前alpha=0.0，导致一直有nan出现，但现在令alpha=0.2，就暂时没有了，很奇怪的问题，为0导致了actor没有了探索，导致了Q值一直增大。
但是在线训练，alpha=0.0没事，也就是说CQL项需要探索来抑制Q值，否则会一直扩大。这样推理的话，如果CQL的alpha=0.0，也会出问题。
在CQL实验，确实Q值增大到不符合常理，也到了700多(正常200多)，但是没有出现nan的现象，最后它居然稳定在了700多，可能继续往后面训练会出问题，但是先到这吧。



SO2

似乎也没啥问题，达到了宣称的性能。

这里我设置的alpha=0.0，但是没影响性能。

