离线强化学习的数据集还是太整齐了

大家后面基本用d4rl进行实验，但是没有按比例分配去看看多少专家数据可以保证让agent恢复到专家水平。




IQL

这里发现用v4版本的环境，收敛到最后不如v2，也就是说，你用v2的数据，就在v2的环境里面实验。

所以我自己电脑上，就只有随便跑跑了，没法达到最佳性能，比如hopper环境中，从v4换到v2，d4rl_score从50%到了66%。

IQL，主要的特点是：1、对value更新用分位数的方式，将adv>0的给予更高的步长，而对动作更新，将adv大的，步长更大，主要是这两点。



CQL

主要思想就是对没有见过的动作进行保守估计，然后对见过的动作进行放大，从而提高整体的估计质量


EDAC

通过多个网络堆叠，利用GPU进行采样

在跑halfcheeth时出了点问题，当用上它新加的gradloss时，半天看不到收敛，他这个类似正则化的效果，至少跑了100epoches，收敛不了，
不过似乎不加这个也不能很快跑出来。。。

然后EDAC主打的应该是walk2d以及hopper，这两个跑着应该没问题


DT

这是一个新的框架，要注意如何处理state，action等，将他们转换为序列，以及transformer怎么设计的，搞明白这些，他没有什么特别的东西。
但是我这里遇到了复现的问题，从数据处理到训练，我觉得应该是一样了，但不知道忽略了哪里，导致达不到峰值性能，但又不是不能训练，很奇怪的问题，
具体的表现是：loss下降的比参考源码更快，但是这种快，带来的是性能的损伤，以及不能稳定的收敛，猜测是过拟合？
然后评估的结果也有点奇怪，我的结果很整齐，标准差很小，回合步长都是稳定的1000(第一轮)，而他的回合步长却均值在600左右。
这里找到问题了，在处理timestep时，我弄错了，没有用1000(mujoco)去判断，写成20(序列长度)了。