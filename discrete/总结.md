C51


1、将Q值变成atoms个分布，他的做法，预先描述整个值分布，比如，从-300到300，51列
然后用奖励去修改每列的值，得到的这个值，作为标签。然后神经网络输出的是一个概率分布，它通过这个标签值以及固定的值分布(-300,300)得到概率分布，在通过这个标签概率分布去更新神经网络。

这个方法很抽象，但是在一些游戏上表现的较好，可能是增加一些探索。

对C51又有了深入的理解，C51更多是基于一个状态下，不同动作的比较，把一个奖励信号反复的在预先设置好的值分布上使用，
然后它学到的不是真值(当然可以调整vmin和vmax得到真值)，总之很重要的是把奖励信号反复的传递了，不过可能在很多好动作上，会出现没法决定的情况。


DDQN


1、思想来源于doubleqlearning，将选择动作和对动作价值的估计的操作分开到两个网络。

2、目的是用于减少过估计，不过这个点需要实验才能深刻体会。

3、突然想到，让选择动作的变成Q网络，这不就是要直接修正Q网络的判断吗？如果是让target网络选择动作并评估，实际上并没有直接影响到Q网络，
只是传递未来可能的动作的价值，但是未来Q网络可能面对下一个状态不会选择target网络的动作，这才是问题所在？



什么是DQN？

1、从开创性的角度：它提出了能解决atari57问题的方法——将神经网络与强化学习结合，在大多数游戏中达到了超人的水平。

2、怎么辨识DQN：回放池，target网络。其中它的基本思想继承Q-learning，得到target-value时，对下一个状态的动作和值的评估都是由target网络做的，
这也为后续的改进留下了空间。



DuelingDQN


1、特点就是，将一个网络直接得到的Q值分割成了V和A的相加，为什么这么做？

作者说可以增加泛化。


2、有趣点是，作者可能一开始做实验发现比不上直接用Prior的DQN，后面又加在PriorDQN上面看看。



nstepDQN

1、主要是用一个固定步数得到未来的奖励累计折扣和，之前我搞错了，用的不是多步之后的状态，虽然不影响性能，反倒表现的还可以(为什么？)

然后后面有试了下直接累加到回合结束，但是又错了，还是用的当前状态的下一个状态，不过打上了折扣，但也没影响性能。

细想一下，这个方法（加上直接的后续状态打上折扣）可以减少方差，其实不算是错的，因为确实也没影响性能。


所有光是一个nstep就有很多分支了。当然这里复现的是教科书的做法，即取n步之后的状态和mask。


效果有点差，然后我看周老师的实现，是没有考虑n步之后的状态的,这次我认为写对了，但是效果仍然不好，真是奇怪。3步的效果要稍微好些，
之前我也写过n步的，那是在SAC的最初的实现中完成的，但是现在来看，可能写错了，但当时却又跑出来了，真是奇怪啊。


现在想起来，nstep，是通过累加当前回合某个状态出发后续的奖励来降低偏差，而每一步都是当时的策略决定的，从这里出发，nstepDDQN可能会更好(实验表明确实，细细分析就是，target网络放大了方差，但又没减少偏差)，因为nstepDQN，没有根据自己未来的策略(没有自己选择动作)来更新。所以看多步的DQN写对没有，其实就是看训练过程中是否有方差。

在rainbow里面，是用的n步之后的step，所以还是改改吧

目前做了一个实验，我记得之前固定随机种子，多步是可以很快收敛的，我在这里试试,确实能收敛，说明在太随机的环境中，多步没啥用，反倒阻碍收敛。
所以要对环境进行描述，当然也可以直接调参。然后，让人惊讶的是，不用n步之后的状态，只用正常的下一个状态，效果更好，可能是没有破坏真值的原因。

总结一下，在lunarlander的环境中，一步更好，特别是对于不固定随机种子的情况，但是对于多步，要用，也不要转换为多步的state。



noiseDQN

1、之前做的是错的，论文里面是直接用最大的Q值的动作作为选择，没有使用epsilongreedy或者softmax采样等方法

2、这里直接构造一个噪音层，每次训练都会改变一部分参数，然后保留一部分参数可学习，增加了泛化。


效果有点差，看RL-adventure，nosie并没有在选择动作的时候启动，只在训练的时候启动；而周老师的RL，是每次使用网络前的时候都启动噪声。

注意这里初始化有个std，默认的是0.4



prioDQN


1、这次主要是加上线段树实现的优先级回放

2、优先级回放加在哪里似乎都能提升性能



QRDQN

1、印象里这个是对奖励函数的极致利用,然后里面有个分位数的概念，具体是体现在计算loss这里

2、首先是抛弃了C51的先验对价值的数值估计，直接让所有分布都是对真实价值的估计，然后取平均值来判断真实的价值，有点像采样，
C51看结果，似乎对一些奖励稀疏的游戏更在行，而QRDQN由于是很利用，很依赖奖励信号，所以对那些奖励密集的游戏更在行。

3、一个关键的操作，将target视作atoms个标签，这些标签每个都更新atoms个目标值，总共atoms*atoms次



DRQN

1、用上了LSTM，相当于在求一个多个状态的条件概率，在实现过程中，要考虑回访池的设计，初始状态的设计，论文里面给了两种方式，后续R2D2也有不同的设计。

2、这个方法的优势应该是能够考虑历史信息，更好的收敛到一个稳定的动作序列上。

3、关于LSTM，还是不熟悉啊，LSTM的输入是状态和hidden(hidden,cell)，输出是lstmout,(hidden,out)，lstmout是所有序列的预测，如果一条序列长为8，
那么会有8个预测，一般用lstmout的最后一个预测，而hidden和cell都是循环神经网络中的属性——相当于，所以，记住了lstm传入下一个层是lstm_out[:,-1,:]。

4、关于DRQN，我目前没有记录hidden和cell，也就是说，训练和采样都没有用上历史信息，只有一个提取信息的模式；有很多方法，比如预热，让16长的序列，前8用于生成
hidden和cell，然后后8在做正事(不过训练和采样具体怎么做的还得想想)；记录hidden和cell，相比于什么都不放，在训练和采样时，放一下hidden和cell，但是没有预热，
或者说预热阶段也被放在采样和训练里面了。



HDQN

1、作者想要的是，元控制器自己筛选目标，然后用这些目标去引导控制器，但问题在于，逻辑链条长度未知，你怎么让控制器穷举到你要的状态，
只有人一步步的像引导动物，丢零食，这样做，这个一步步，是问题的核心，作者是自己对图像进行处理，这样子，太专一了；

2、最后的贡献就是一个思路，或者说谁都能想到的，我们需要对每个状态进行评估价值，获得高价值的状态，然后用这个目标引导控制器去达成这个状态，
但问题在于，稀疏的奖励没法探索得到哪个状态价值高，而稠密的奖励又不需要你这样引导，所以很尴尬；

3、综上，我不会复现这个算法了，它仅仅提供了一个思路，但是自己都没做出来，走了捷径，后续的RND，NGU算是集大成者，也就是内在奖励引导探索，但这些
问题的根本还是状态的编码，或者说一连串状态的编码，标签(结合语言模型)，只有这样才能达到泛用。




Rainbow和QRRainbow

就是排列组合的终极版本，不过好像也没啥用....

这里对duel的处理有点问题，但是rainbow没受影响，猜测是C51学到的是概率分布，输出前要做一个softmax，所以q = v + a - a.mean(-1,keepdim=True)这个不影响，相当于做了一个缩放，并没有改变相对大小；但是QR-rainbow就不行了，因为这里学到的是真值，每个动作的分布减去一个平均值，然后输出的动作取决于单个动作分布的平均值的相对大小，此时减去分布平均值的操作会改变相对大小，所以不行。

q = v + a - a.mean(1,keepdim=True)，还是这一步，主要是QRrainbow，这里的平均是对同样定位的分布进行平均的，比如，4个动作，每个动作51个采样，正确的做法是51个采样从1/51一直加到1，也就是最后一个格子贡献1，其他的都是累加上来的，如果对这些格子进行排序，此时用duel，应该会出错。



ppo

onpolicy的版本，ppo，主要是针对策略进行优化的，具体的体现是对logprob进行增减.

熵的理解，一开始是最小化：-P*logP,这里是在让后项-logP接近0，P接近1，而前项P是接近0，这里搞错了，这里的优化是矛盾的，正确的是，最小化PlogP，此时logP接近负无穷，P接近0。很矛盾的一点，P如果真的要最小化这个整体应该接近1才对。正确与否我是看的实验结果看的，然后从结果来看，熵项应该是提供随机性，也就是让P接近0，而不是1，也不是让它们矛盾打架。我想错了，熵这个本来就是越高越随机，所以应该最小化PlogP，让它提供随机。

onpolicy表现得还行

offpolicy果然不行，我在测试将回放池大小从125-20-10，看看变化



SAC

主要是把握熵和软更新


SAC和TD3都面临着梯度无法通过动作传递到actor网络，这里根据离散采样和离散确定分为两种方法：Gumbel-Softmax和(Softmax 与 Straight-Through)
前者是通过采样得到动作，后者是直接选取概率最大的动作。

这里将不再采用SAC和TD3Q(s,a)的方式得到动作价值了，而是通过Q(s),Pi(s)分别得到n个动作的价值和概率，为什么不用前者，首先前者在离散动作中，如何通过Q网络更新
actor是个大问题，如何通过Q告诉actor，这个logit该增加或者减少，如果action用正常的序号或者对序号进行onehot，他们都是固定的离散整数，然后损失函数只有极大化Q函数这一种可能，它能传递的就是我要增加这个值，或者减少这个值，在这个输出下我最终的价值才会更大，但是问题在于这些数是固定的，陷入了矛盾，因为在Q函数眼中action应该是所有实数，就感觉好别扭，暂时想不到解决办法了。所以用后者，后者是参考一篇论文。
但也可以做一些实验：比如将action替换为logit或者prob会怎么样？
这里碰到一个奇葩的问题：self.alpha的更新，如果在过程中self.alpha.to(device)，将导致梯度传递失败，记住了！


TD3

主要是actor延迟更新，以及用固定的策略去探索(这里也是可以改的地方)
他的特点是：1、延迟更新actor和target网络；2、双Q网络缓解过估计；3、在更新Q时用噪声平滑周围的值
