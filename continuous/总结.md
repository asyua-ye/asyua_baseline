mujoco环境


加了多进程之后，效果不太理想，所以放弃了，主要也不知道这里多进程能起啥作用；

分析下原因：训练节奏的问题，每次采样8000步，训练6000步，在这里行不通，在我之前写好的设置下是单进程，先跑1万步，然后后面每跑50步，训练50次，
这两个训练节奏，本质区别是模型的更新导致经验序列版本前者迭代很慢，后者迭代更快，这应该是原因。


多进程，主要是为了多采样，这里似乎不需要。


然后联想到多进程，似乎可以同时训练多个环境，因为这些环境的模型都长一个样，输入和输出也可以用0补齐。

在重写SAC的时候，遇到了复现结果不对的情况，一开始是在找不同，其实最具效率的做法，是在一个已经实验好的代码上与现在新写的代码进行比较，
通过修改前者，来找问题。

找到问题了：LambdaLR(self.actor_optimizer, lr_lambda=lambda_lr)

这段代码，注释了就好了，现在去看看为什么？ 这段代码在初始化的时候就会开始影响优化器，后续不调用会不会影响，我不确定，但是能肯定的是，
只要初始化了，就会影响优化器，导致训练整体变形，在这里，我的推断是，会一直影响，这里能做出一个猜测，学习率递减对于SAC作用不大，甚至是副作用。

上面的猜测是错的，发现是学习率的问题，确实LambdaLR仅仅只是走了一步，也就是默认的step为1，导致学习率从3e-4变为了2.9999999999999997e-05，
而这个学习率，有问题。这个学习率最接近的是3e-5，实验发现：确实有问题，所以根本问题是lambdalr在初始化的时候对3e-4 * 0.1，导致了学习率变化，导致了后续
一系列问题。

但是我在离散环境中测试了下，发现又没有这个问题...



所以问题出在，为什么初始化，学习率会这样？发现了，我设置的max_epoches以及total_epoches分别是1e5和1e6，这里正好是0.1的关系，满足我写的lambdalr，
所以问题在这里。总结一下：LambdaLR会初始化判断，然后改变学习率，此后不调用不再改变学习率，然后3e-5不适合SAC任务。

对了，对于mojoco环境，训练过程中不要开humman模式，采样太慢了

对于SAC，性能主要看梯度更新了多少次，然后对比论文里面的曲线，不过自适应的alpha不符合论文里的曲线，很奇怪。
